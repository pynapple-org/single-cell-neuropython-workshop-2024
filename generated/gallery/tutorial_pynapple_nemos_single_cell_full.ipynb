{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete tutorial pynapple & NeMoS\n\nFor our first example, we will look at a very simple dataset: patch-clamp\nrecordings from a single neuron in layer 4 of mouse primary visual cortex. This\ndata is from the [Allen Brain\nAtlas](https://celltypes.brain-map.org/experiment/electrophysiology/478498617),\nand experimenters injected current directly into the cell, while recording the\nneuron's membrane potential and spiking behavior. The experiments varied the\nshape of the current across many sweeps, mapping the neuron's behavior in\nresponse to a wide range of potential inputs.\n\nFor our purposes, we will examine only one of these sweeps, \"Noise 1\", in which\nthe experimentalists injected three pulses of current. The current is a square\npulse multiplied by a sinusoid of a fixed frequency, with some random noise\nriding on top.\n\n![Allen Brain Atlas view of the data we will analyze.](../../assets/allen_data.png)\n\nIn the figure above (from the Allen Brain Atlas website), we see the\napproximately 22 second sweep, with the input current plotted in the first row,\nthe intracellular voltage in the second, and the recorded spikes in the third.\n(The grey lines and dots in the second and third rows comes from other sweeps\nwith the same stimulus, which we'll ignore in this exercise.) When fitting the\nGeneralized Linear Model, we are attempting to model the spiking behavior, and\nwe generally do not have access to the intracellular voltage, so for the rest\nof this notebook, we'll use only the input current and the recorded spikes\ndisplayed in the first and third rows.\n\nFirst, let us see how to load in the data and reproduce the above figure, which we'll do\nusing [pynapple](https://pynapple-org.github.io/pynapple/). This will largely be a\nreview of what we went through yesterday. After we've explored the data some, we'll\nintroduce the Generalized Linear Model and how to fit it with NeMoS.\n\n\n<div class=\"notes\">\nData for this notebook is a patch clamp experiment with a mouse V1 neuron, from the [Allen Brain Atlas](https://celltypes.brain-map.org/experiment/electrophysiology/478498617)\n\n![Allen Brain Atlas view of the data we will analyze.](../../assets/allen_data.png)\n</div>\n\n## Learning objectives {.keep-text}\n\n- Learn how to explore spiking data and do basic analyses using pynapple\n- Learn how to structure data for NeMoS\n- Learn how to fit a basic Generalized Linear Model using NeMoS\n- Learn how to retrieve the parameters and predictions from a fit GLM for\n  intrepetation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! warning\n    This tutorial uses matplotlib for displaying the figure\n\n    You can install all with `pip install matplotlib requests tqdm`\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install matplotlib requests tqdm\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you did not install beforehand pynapple and nemos, here is the command to install it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !pip install pynapple nemos\n\nimport math\nimport os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import everything\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import jax\nimport matplotlib.pyplot as plt\nimport nemos as nmo\nimport numpy as np\nimport pynapple as nap\nimport requests\nimport tqdm\nimport workshop_utils.plotting as plotting\n\n# configure plots some\nplt.style.use(\"workshop_utils/nemos.mplstyle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Streaming\n\nWhile you can download the data directly from the Allen Brain Atlas and\ninteract with it using their\n[AllenSDK](https://allensdk.readthedocs.io/en/latest/visual_behavior_neuropixels.html),\nwe prefer the burgeoning [Neurodata Without Borders (NWB)\nstandard](https://nwb-overview.readthedocs.io/en/latest/). We have converted\nthis single dataset to NWB and uploaded it to the [Open Science\nFramework](https://osf.io/5crqj/). This allows us to easily load the data\nusing pynapple, and it will immediately be in a format that pynapple understands!\n\n!!! tip\n\n    Pynapple can stream any NWB-formatted dataset! See [their\n    documentation](https://pynapple-org.github.io/pynapple/generated/gallery/tutorial_pynapple_dandi/)\n    for more details, and see the [DANDI Archive](https://dandiarchive.org/)\n    for a repository of compliant datasets.\n\nThe first time the following cell is run, it will take a little bit of time\nto download the data, and a progress bar will show the download's progress.\nOn subsequent runs, the cell gets skipped: we do not need to redownload the\ndata.\n<div class=\"notes\">\n- Stream the data. Format is [Neurodata Without Borders (NWB) standard](https://nwb-overview.readthedocs.io/en/latest/)\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = \"allen_478498617.nwb\"\nif path not in os.listdir(\".\"):\n  r = requests.get(f\"https://osf.io/vf2nj/download\", stream=True)\n  block_size = 1024*1024\n  with open(path, 'wb') as f:\n    for data in tqdm.tqdm(r.iter_content(block_size), unit='MB', unit_scale=True,\n      total=math.ceil(int(r.headers.get('content-length', 0))//block_size)):\n      f.write(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pynapple\n\n### Data structures and preparation\n\nNow that we've downloaded the data, let's open it with pynapple and examine\nits contents.\n\n<div class=\"notes\">\n- Open the NWB file with [pynapple](https://pynapple-org.github.io/pynapple/)\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = nap.load_file(path)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains several different pynapple objects, which we discussed yesterday.\nLet's see how these relate to the data we visualized above:\n\n![Annotated view of the data we will analyze.](../../assets/allen_data_annotated.gif)\n<!-- this gif created with the following imagemagick command: convert -layers OptimizePlus -delay 100 allen_data_annotated-units.svg allen_data_annotated-epochs.svg allen_data_annotated-stimulus.svg allen_data_annotated-response.svg -loop 0 allen_data_annotated.gif -->\n\n- `units`: dictionary of neurons, holding each neuron's spike timestamps.\n- `epochs`: start and end times of different intervals, defining the\n  experimental structure, specifying when each stimulation protocol began and\n  ended.\n- `stimulus`: injected current, in Amperes, sampled at 20k Hz.\n- `response`: the neuron's intracellular voltage, sampled at 20k Hz.\n  We will not use this info in this example\n\nNow let's go through the relevant variables in some more detail:\n<div class=\"notes\">\"\n![Annotated view of the data we will analyze.](../../assets/allen_data_annotated.gif)\n\n- `stimulus`: Tsd containing injected current, in Amperes, sampled at 20k Hz.\n- `response`: Tsd containing the neuron's intracellular voltage, sampled at 20k Hz.\n- `units`: Tsgroup, dictionary of neurons, holding each neuron's spike timestamps.\n- `epochs`: IntervalSet, dictionary with start and end times of different intervals,\n  defining the experimental structure, specifying when each stimulation protocol began\n  and ended.\n</div>\n\nFirst, let's examine the epochs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = data[\"epochs\"]\nepochs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`epochs` is a dictionary with strings for keys and\n[`IntervalSets`](https://pynapple-org.github.io/pynapple/reference/core/interval_set/)\nfor values. Each key defines the stimulus protocol, with the value defining\nthe beginning and end of that stimulation protocol.\n\n<div class=\"notes\">\"\n- `Noise 1`: epochs of random noise\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_interval = epochs[\"Noise 1\"]\nnoise_interval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As described above, we will be examining \"Noise 1\". We can see it contains\nthree rows, each defining a separate sweep. We'll just grab the first sweep\n(shown in blue in the pictures above) and ignore the other two (shown in\ngray).\n\n<div class=\"notes\">\"\n- Let's focus on the first epoch.\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_interval = noise_interval[0]\nnoise_interval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's examine the input current:\n\n<div class=\"notes\">\"\n- `current` : Tsd (TimeSeriesData) : time index + data\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# convert current from Ampere to pico-amperes, to match the Allen Institute figures and\n# move the values to a more reasonable range.\ncurrent = data[\"stimulus\"] * 1e12\ncurrent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`current` is a `Tsd`\n([TimeSeriesData](https://pynapple-org.github.io/pynapple/reference/core/time_series/))\nobject with 2 columns. Like all `Tsd` objects, the first column contains the\ntime index and the second column contains the data; in this case, the current\nin pA.\n\nCurrently `current` contains the entire ~900 second experiment but, as\ndiscussed above, we only want one of the \"Noise 1\" sweeps. Fortunately,\n`pynapple` makes it easy to grab out the relevant time points by making use\nof the `noise_interval` we defined above:\n\n<div class=\"notes\">\"\n- `restrict` : restricts a time series object to a set of time intervals delimited by an IntervalSet object\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "current = current.restrict(noise_interval)\ncurrent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the timestamps have changed and our shape is much smaller.\n\nFinally, let's examine the spike times. These are stored in a\n[`TsGroup`](https://pynapple-org.github.io/pynapple/reference/core/ts_group/),\na dictionary-like object that holds multiple `Ts` (timeseries) objects with\npotentially different time indices:\n\n<div class=\"notes\">\"\n- `TsGroup` : a custom dictionary holding multiple `Ts` (timeseries) objects with\n  potentially different time indices.\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spikes = data[\"units\"]\nspikes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Typically, this is used to hold onto the spike times for a population of\nneurons. In this experiment, we only have recordings from a single neuron, so\nthere's only one row.\n\nWe can index into the `TsGroup` to see the timestamps for this neuron's\nspikes:\n\n<div class=\"notes\">\"\nWe can index into the `TsGroup` to see the timestamps for this neuron's\nspikes:\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spikes[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to `current`, this object originally contains data from the entire\nexperiment. To get only the data we need, we again use\n`restrict(noise_interval)`:\n\n<div class=\"notes\">\"\nLet's restrict to the same epoch `noise_interval`:\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spikes = spikes.restrict(noise_interval)\nprint(spikes)\nspikes[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's visualize the data from this trial, replicating rows 1 and 3\nfrom the Allen Brain Atlas figure at the beginning of this notebook:\n\n<div class=\"notes\">\"\nLet's visualize the data from this trial:\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 2))\nax.plot(current, \"grey\")\nax.plot(spikes.to_tsd([-5]), \"|\", color=\"k\", ms = 10)\nax.set_ylabel(\"Current (pA)\")\nax.set_xlabel(\"Time (s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic analyses\n\nBefore using the Generalized Linear Model, or any model, it's worth taking some time\nto examine our data and think about what features are interesting and worth capturing.\nAs Edoardo explained earlier today, the GLM is a model of the neuronal firing rate.\nHowever, in our experiments, we do not observe the firing rate, only the spikes!\nMoreover, neural responses are typically noisy &mdash; even in this highly controlled\nexperiment where the same current was injected over multiple trials, the spike times\nwere slightly different from trial-to-trial. No model can perfectly predict spike\ntimes on an individual trial, so how do we tell if our model is doing a good job?\n\nOur objective function is the log-likelihood of the observed spikes given the\npredicted firing rate. That is, we're trying to find the firing rate, as a\nfunction of time, for which the observed spikes are likely. Intuitively, this\nmakes sense: the firing rate should be high where there are many spikes, and\nvice versa. However, it can be difficult to figure out if your model is doing\na good job by squinting at the observed spikes and the predicted firing rates\nplotted together. \n\nOne common way to visualize a rough estimate of firing rate is to smooth\nthe spikes by convolving them with a Gaussian filter.\n\n!!! info\n\n    This is a heuristic for getting the firing rate, and shouldn't be taken\n    as the literal truth (to see why, pass a firing rate through a Poisson\n    process to generate spikes and then smooth the output to approximate the\n    generating firing rate). A model should not be expected to match this\n    approximate firing rate exactly, but visualizing the two firing rates\n    together can help you reason about which phenomena in your data the model\n    is able to adequately capture, and which it is missing.\n\n    For more information, see section 1.2 of [*Theoretical\n    Neuroscience*](https://boulderschool.yale.edu/sites/default/files/files/DayanAbbott.pdf),\n    by Dayan and Abbott.\n\nPynapple can easily compute this approximate firing rate, and plotting this\ninformation will help us pull out some phenomena that we think are\ninteresting and would like a model to capture.\n\nFirst, we must convert from our spike times to binned spikes:\n\n<div class=\"notes\">\"\nThe Generalized Linear Model gives a predicted firing rate. First we can use\npynapple to visualize this firing rate for a single trial.\n\n- `count` : count the number of events within `bin_size`\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# bin size in seconds\nbin_size = 0.001\n# Get spikes for neuron 0\ncount = spikes[0].count(bin_size)\ncount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's convert the binned spikes into the firing rate, by smoothing them\nwith a gaussian kernel. Pynapple again provides a convenience function for\nthis:\n<div class=\"notes\">\"\nLet's convert the spike counts to firing rate :\n\n- `smooth` : convolve with a Gaussian kernel\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the inputs to this function are the standard deviation of the gaussian in seconds and\n# the full width of the window, in standard deviations. So std=.05 and size_factor=20\n# gives a total filter size of 0.05 sec * 20 = 1 sec.\nfiring_rate = count.smooth(std=.05, size_factor=20)\n# convert from spikes per bin to spikes per second (Hz)\nfiring_rate = firing_rate / bin_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've done all this preparation, let's make a plot to more easily\nvisualize the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# we're hiding the details of the plotting function for the purposes of this\n# tutorial, but you can find it in the associated github repo if you're\n# interested:\n# https://github.com/flatironinstitute/ccn-workshop-fens-2024/blob/main/src/workshop_utils/plotting.py\nplotting.current_injection_plot(current, spikes, firing_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So now that we can view the details of our experiment a little more clearly,\nwhat do we see?\n\n- We have three intervals of increasing current, and the firing rate\n  increases as the current does.\n\n- While the neuron is receiving the input, it does not fire continuously or\n  at a steady rate; there appears to be some periodicity in the response. The\n  neuron fires for a while, stops, and then starts again. There's periodicity\n  in the input as well, so this pattern in the response might be reflecting\n  that.\n\n- There's some decay in firing rate as the input remains on: there are three or\n  four \"bumps\" of neuronal firing in the second and third intervals and they\n  decrease in amplitude, with the first being the largest.\n\nThese give us some good phenomena to try and predict! But there's something\nthat's not quite obvious from the above plot: what is the relationship\nbetween the input and the firing rate? As described in the first bullet point\nabove, it looks to be *monotonically increasing*: as the current increases,\nso does the firing rate. But is that exactly true? What form is that\nrelationship?\n\nPynapple can compute a tuning curve to help us answer this question, by\nbinning our spikes based on the instantaneous input current and computing the\nfiring rate within those bins:\n\n<div class=\"notes\">\"\nWhat is the relationship between the current and the spiking activity?\n[`compute_1d_tuning_curves`](https://pynapple-org.github.io/pynapple/reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves) : compute the firing rate as a function of a 1-dimensional feature.\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tuning_curve = nap.compute_1d_tuning_curves(spikes, current, nb_bins=15)\ntuning_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`tuning_curve` is a pandas DataFrame where each column is a neuron (one\nneuron in this case) and each row is a bin over the feature (here, the input\ncurrent). We can easily plot the tuning curve of the neuron:\n\n<div class=\"notes\">\"\nLet's plot the tuning curve of the neuron.\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.tuning_curve_plot(tuning_curve)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that, while the firing rate mostly increases with the current,\nit's definitely not a linear relationship, and it might start decreasing as\nthe current gets too large.\n\nSo this gives us three interesting phenomena we'd like our model to help\nexplain: the tuning curve between the firing rate and the current, the firing\nrate's periodicity, and the gradual reduction in firing rate while the\ncurrent remains on.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NeMoS {.strip-code}\n\n### Preparing data\n\nNow that we understand our model, we're almost ready to put it together.\nBefore we construct it, however, we need to get the data into the right\nformat.\n\nNeMoS requires that the predictors and spike counts it operates on have the\nfollowing properties:\n\n- predictors and spike counts must have the same number of time points.\n\n- predictors must be two-dimensional, with shape `(n_time_bins, n_features)`.\n  In this example, we have a single feature (the injected current).\n\n- spike counts must be one-dimensional, with shape `(n_time_bins, )`. As\n  discussed above, `n_time_bins` must be the same for both the predictors and\n  spike counts.\n\n- predictors and spike counts must be\n  [`jax.numpy`](https://jax.readthedocs.io/en/latest/jax-101/01-jax-basics.html)\n  arrays, `numpy` arrays, or `pynapple` `TsdFrame`/`Tsd`.\n\n!!! info \"What is jax?\"\n\n    [jax](https://github.com/google/jax) is a Google-supported python library\n    for automatic differentiation. It has all sorts of neat features, but the\n    most relevant of which for NeMoS is its GPU-compatibility and\n    just-in-time compilation (both of which make code faster with little\n    overhead!), as well as the collection of optimizers present in\n    [jaxopt](https://jaxopt.github.io/stable/).\n\nFirst, we require that our predictors and our spike counts have the same\nnumber of time bins. We can achieve this by down-sampling our current to the\nspike counts to the proper resolution using the\n[`bin_average`](https://pynapple-org.github.io/pynapple/reference/core/time_series/#pynapple.core.time_series.BaseTsd.bin_average)\nmethod from pynapple:\n\n!!! info\n\n    We refer to the model inputs as both \"the predictors\" and \"the design matrix,\" a\n    term which comes from statistics.\n\n<div class=\"notes\">\n Get data from pynapple to NeMoS-ready format:\n\n  - predictors and spikes must have same number of time points\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "binned_current = current.bin_average(bin_size)\n\nprint(f\"current shape: {binned_current.shape}\")\n# rate is in Hz, convert to KHz\nprint(f\"current sampling rate: {binned_current.rate/1000.:.02f} KHz\")\n\nprint(f\"\\ncount shape: {count.shape}\")\nprint(f\"count sampling rate: {count.rate/1000:.02f} KHz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secondly, we have to reshape our variables so that they are the proper shape:\n\n- `predictors`: `(n_time_bins, n_features)`\n- `count`: `(n_time_bins, )`\n\nBecause we only have a single predictor feature, we'll use\n[`np.expand_dims`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html)\nto ensure it is a 2d array.\n\n<div class=\"notes\">\n  - predictors must be 2d, spikes 1d\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# make sure predictor is 2d\npredictor = np.expand_dims(binned_current, 1)\n\n# check that the dimensionality matches NeMoS expectation\nprint(f\"predictor shape: {predictor.shape}\")\nprint(f\"count shape: {count.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! info \"What if I have more than one neuron?\"\n\n    In this example, we're only fitting data for a single neuron, but you\n    might wonder how the data should be shaped if you have more than one\n    neuron &mdash; do you add an extra dimension? or concatenate neurons along one\n    of the existing dimensions?\n\n    In NeMoS, we fit Generalized Linear Models to a single neuron at a time. We'll\n    discuss this more in the [following tutorial](../06_head_direction/), but briefly:\n    you get the same answer whether you fit the neurons separately or simultaneously,\n    and fitting them separately can make your life easier. We also provide a\n    `PopulationGLM` object to fit an entirely population at once, if you prefer to do\n    so.\n\n### Fitting the model\n\nNow we're ready to fit our model!\n\nFirst, we need to define our GLM model object. We intend for users to interact with\nour models like [scikit-learn](https://scikit-learn.org/stable/getting_started.html)\nestimators. In a nutshell, a model instance is initialized with hyperparameters that\nspecify optimization and model details, and then the user calls the `.fit()` function\nwith the design matrix and the observed data to fit the model. We will walk you\nthrough the process below by example, but if you are interested in reading more\ndetails see the [Getting Started with\nscikit-learn](https://scikit-learn.org/stable/getting_started.html) webpage.\n\nTo initialize our model, we need to specify the regularizer and observation\nmodel objects, both of which should be one of our custom objects:\n\n- Regularizer: this object specifies both the solver algorithm and the\n  regularization scheme. They are jointly specified because each\n  regularization scheme has a list of compatible solvers to choose between.\n  Regularization modifies the objective function to reflect your prior\n  beliefs about the parameters, such as sparsity. Regularization becomes more\n  important as the number of input features, and thus model parameters,\n  grows. They can be found within `nemos.regularizer`.\n\n!!! warning\n\n    With a convex problem like the GLM, in theory it does not matter which solver\n    algorithm you use. In practice, due to numerical issues, it generally does. Thus,\n    it's worth trying a couple to see how their solutions compare. (Note that, since\n    regularization modifies the objective function, different regularization schemes\n    will always give different results.)\n\n- Observation model: this object links the firing rate and the observed data (in this\n  case, spikes), describing the distribution of neural activity (and thus changing the\n  log-likelihood). For spiking data, we use the Poisson observation model, but nemos\n  provides other options for continuous data, such as calcium imaging.\n\nFor this example, we'll use an un-regularized LBFGS solver. We'll discuss\nregularization in a later tutorial.\n\n!!! info \"Why LBFGS?\"\n\n    [LBFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) is a\n    quasi-Netwon method, that is, it uses the first derivative (the gradient)\n    and approximates the second derivative (the Hessian) in order to solve\n    the problem. This means that LBFGS tends to find a solution faster and is\n    often less sensitive to step-size. Try other solvers to see how they\n    behave!\n\n<div class=\"notes\">\n  - GLM objects need regularizers and observation models\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = nmo.glm.GLM(solver_name=\"LBFGS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've initialized our model with the optimization parameters, we can\nfit our data! In the previous section, we prepared our model matrix\n(`predictor`) and target data (`count`), so to fit the model we just need to\npass them to the model:\n\n<div class=\"notes\">\n  - call fit and retrieve parameters\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.fit(predictor, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we've fit our data, we can retrieve the resulting parameters.\nSimilar to scikit-learn, these are stored as the `coef_` and `intercept_`\nattributes:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"firing_rate(t) = exp({model.coef_} * current(t) + {model.intercept_})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that `model.coef_` has shape `(n_features, )` and `model.intercept_` has shape\n`(n_neurons, )` (in this case, both are 1):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"coef_ shape: {model.coef_.shape}\")\nprint(f\"intercept_ shape: {model.intercept_.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's nice to get the parameters above, but we can't tell how well our model\nis doing by looking at them. So how should we evaluate our model?\n\nFirst, we can use the model to predict the firing rates and compare that to\nthe smoothed spike train. By calling `predict()` we can get the model's\npredicted firing rate for this data. Note that this is just the output of the\nmodel's linear and nonlinear steps, as described in Edoardo's presentation!\n\n<div class=\"notes\">\n  - generate and examine model predictions.\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_fr = model.predict(predictor)\n# convert units from spikes/bin to spikes/sec\npredicted_fr = predicted_fr / bin_size\n\n# and let's smooth the firing rate the same way that we smoothed the\n# spike train\nsmooth_predicted_fr = predicted_fr.smooth(.05, size_factor=20)\n\n# and plot!\nplotting.current_injection_plot(current, spikes, firing_rate,\n                                               smooth_predicted_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What do we see above? Note that the y-axes in the final row are different for\neach subplot!\n\n- Predicted firing rate increases as injected current goes up &mdash; Success! :tada:\n\n- The amplitude of the predicted firing rate only matches the observed\n  amplitude in the third interval: it's too high in the first and too low in\n  the second &mdash; Failure! :x:\n\n- Our predicted firing rate has the periodicity we see in the smoothed spike\n  train &mdash; Success! :tada:\n\n- The predicted firing rate does not decay as the input remains on: the\n  amplitudes are identical for each of the bumps within a given interval &mdash;\n  Failure! :x:\n\nThe failure described in the second point may seem particularly confusing &mdash;\napproximate amplitude feels like it should be very easy to capture, so what's\ngoing on?\n\nTo get a better sense, let's look at the mean firing rate over the whole\nperiod:\n\n<div class=\"notes\">\n  - what do we see?\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compare observed mean firing rate with the model predicted one\nprint(f\"Observed mean firing rate: {np.mean(count) / bin_size} Hz\")\nprint(f\"Predicted mean firing rate: {np.mean(predicted_fr)} Hz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We matched the average pretty well! So we've matched the average and the\nrange from the third interval reasonably well, but overshot at low\ninputs and undershot in the middle.\n\nWe can see this more directly by computing the tuning curve for our predicted\nfiring rate and comparing that against our smoothed spike train from the\nbeginning of this notebook. Pynapple can help us again with this:\n\n<div class=\"notes\">\n  - examine tuning curve &mdash; what do we see?\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tuning_curve_model = nap.compute_1d_tuning_curves_continuous(predicted_fr, current, 15)\nfig = plotting.tuning_curve_plot(tuning_curve)\nfig.axes[0].plot(tuning_curve_model, color=\"tomato\", label=\"glm\")\nfig.axes[0].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to making the mismatch at low and medium input values discussed earlier a\nlittle more obvious, this tuning curve comparison also highlights that this model\nthinks the firing rate will continue to grow as the injected current increases, which\nis not reflected in the data.\n\nViewing this plot also makes it clear that the model's tuning curve is\napproximately exponential. We already knew that! That's what it means to be a\nLNP model of a single input. But it's nice to see it made explicit.\n\n### Extending the model\n\nWe can try extending the model in order to improve its performance. There are many\nways one can do this: the iterative refinement and improvement of your model is an\nimportant part of the scientific process! In this tutorial, we'll discuss one such\nextension, but you're encouraged to try others.\n\nOur model right now assumes that the neuron's spiking behavior is only driven by the\n*instantaneous input current*. That is, we're saying that history doesn't matter. But\nwe know that neurons integrate information over time, so why don't we add extend our\nmodel to reflect that?\n\nTo do so, we will change our predictors, including variables that represent the\nhistory of the input current as additional columns. First, we must decide the duration\nof time that we think is relevant: does current passed to the cell 10 msec ago matter?\nwhat about 100 msec? 1 sec? To start, we should use our a priori knowledge about the\nsystem to determine a reasonable initial value. Later, we can examine the model\nparameters and do formal model comparison in order to determine how much history is\nnecessary.\n\nFor now, let's use a duration of 200 msec:\n\n<div class=\"notes\">\n  - choose a length of time over which the neuron integrates the input current\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "current_history_duration_sec = .2\n# convert this from sec to bins\ncurrent_history_duration = int(current_history_duration_sec / bin_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct our new predictors, we could simply take the current and shift it\nincrementally. The value of predictor `binned_current` at time $t$ is the injected\ncurrent at time $t$; by shifting `binned_current` backwareds by 1, we are modeling the\neffect of the current at time $t-1$ on the firing rate at time $t$, and so on for all\nshifts $i$ up to `current_history_duration`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "binned_current[1:]\nbinned_current[2:]\n# etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In general, however, this is not a good way to extend the model in the way discussed.\nYou will end end up with a very large number of predictive variables (one for every\nbin shift!), which will make the model more sensitive to noise in the data.\n\nA better idea is to do some dimensionality reduction on these predictors, by\nparametrizing them using **basis functions**. These will allow us to capture\ninteresting non-linear effects with a relatively low-dimensional parametrization that\npreserves convexity. NeMoS has a whole library of basis objects available at\n`nmo.basis`, and choosing which set of basis functions and their parameters, like\nchoosing the duration of the current history predictor, requires knowledge of your\nproblem, but can later be examined using model comparison tools.\n\nFor history-type inputs like we're discussing, the raised cosine log-stretched basis\nfirst described in Pillow et al., 2005 [^3] is a good fit. This basis set has the nice\nproperty that their precision drops linearly with distance from event, which is a\nmakes sense for many history-related inputs in neuroscience: whether an input happened\n1 or 5 msec ago matters a lot, whereas whether an input happened 51 or 55 msec ago is\nless important.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.plot_basis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[^3]: Pillow, J. W., Paninski, L., Uzzel, V. J., Simoncelli, E. P., & J.,\nC. E. (2005). Prediction and decoding of retinal ganglion cell responses\nwith a probabilistic spiking model. Journal of Neuroscience, 25(47),\n11003\u201311013. http://dx.doi.org/10.1523/jneurosci.3305-05.2005\n\nNeMoS's `Basis` objects handle the construction and use of these basis functions. When\nwe instantiate this object, the main argument we need to specify is the number of\nfunctions we want: with more basis functions, we'll be able to represent the effect of\nthe corresponding input with the higher precision, at the cost of adding additional\nparameters.\n\nWe also need to specify whether we want to use the basis in convolutional (`\"conv\"`)\nor evaluation (`\"eval\"`) mode. This is determined by the type of feature we wish to\nrepresent with the basis:\n\n- Evaluation mode transforms the input through the non-linear function defined by the\n  basis. This can be used to represent features such as spatial location and head\n  direction.\n\n- Convolution mode applies a convolution of the input data to the bank of filters\n  defined by the basis, and is particularly useful when analyzing data with inherent\n  temporal dependencies, such as spike history or the history of input current in this\n  example. In convolution mode, we must additionally specify the `window_size`, the\n  length of the filters in bins.\n\n<div class=\"notes\">\n  - define a basis object\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basis = nmo.basis.RaisedCosineBasisLog(\n    n_basis_funcs=10, mode=\"conv\", window_size=current_history_duration,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! note \"Visualizing `Basis` objects\"\n\n    NeMoS provides some convenience functions for quickly visualizing the basis, in\n    order to create plots like the type seen above.\n\n    ```python\n    # basis_kernels is an array of shape (current_history_duration, n_basis_funcs)\n    # while time is an array of shape (current_history_duration, )\n    time, basis_kernels = basis.evaluate_on_grid(current_history_duration)\n    # convert time to sec\n    time *= current_history_duration_sec\n    plt.plot(time, basis_kernels)\n    ```\n\nWith this basis in hand, we can compress our input features:\n\n<div class=\"notes\">\n  - create the design matrix\n  - examine the features it contains\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# under the hood, this convolves the input with the filter bank visualized above\ncurrent_history = basis.compute_features(binned_current)\nprint(current_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our design matrix is now 28020 time points by 10 features, one for\neach of our basis functions. If we had used the raw shifted data as the features, like\nwe started to do above, we'd have a design matrix with 200 features, so we've ended up\nwith more than an order of magnitude fewer features!\n\nNote that we have a bunch of NaNs at the beginning of each column. That's because of\nboundary handling: we're using the input of the past 200 msecs to predict the firing\nrate at time $t$, so what do we do in the first 200 msecs? The safest way is to ignore\nthem, so that the model doesn't consider them during the fitting procedure.\n\nWhat do these features look like?\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# in this plot, we're normalizing the amplitudes to make the comparison easier --\n# the amplitude of these features will be fit by the model, so their un-scaled\n# amplitudes is not informative\nplotting.plot_current_history_features(binned_current, current_history, basis,\n                                                      current_history_duration_sec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the top row, we're visualizing the basis functions, as above. On the bottom row,\nwe're showing the input current, as a black dashed line, and corresponding features\nover a small window of time, just as the current is being turned on. These features\nare the result of a convolution between the basis function on the top row with the\nblack dashed line shown below. As the basis functions get progressively wider and\ndelayed from the event start, we can thus think of the features as weighted averages\nthat get progressively later and smoother. Let's step through that a bit more slowly.\n\nIn the leftmost plot, we can see that the first feature almost perfectly tracks the\ninput. Looking at the basis function above, that makes sense: this function's max is\nat 0 and quickly decays. This feature is thus a very slightly smoothed version of the\ninstantaneous current feature we were using before. In the middle plot, we can see\nthat the last feature has a fairly long lag compared to the current, and is a good\ndeal smoother. Looking at the rightmost plot, we can see that the other features vary\nbetween these two extremes, getting smoother and more delayed.\n\nThese are the elements of our feature matrix: representations of not just the\ninstantaneous current, but also the current history, with precision decreasing as the\nlag between the predictor and current increases. Let's see what this looks like when\nwe go to fit the model!\n\nWe'll initialize and create the GLM object in the same way as before, only changing\nthe design matrix we pass to the model:\n\n<div class=\"notes\">\n  - create and fit the GLM\n  - examine the parameters\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "history_model = nmo.glm.GLM(solver_name=\"LBFGS\")\nhistory_model.fit(current_history, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As before, we can examine our parameters, `coef_` and `intercept_`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"firing_rate(t) = exp({history_model.coef_} * current(t) + {history_model.intercept_})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the shape of these parameters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(history_model.coef_.shape)\nprint(history_model.intercept_.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`coef_` has 10 values now, while `intercept_` still has one &mdash; why is that?\nBecause we now have 10 features, but still only 1 neuron whose firing rate we're\npredicting.\n\nLet's re-examine our predicted firing rate and see how the new model does:\n\n<div class=\"notes\">\n  - compare the predicted firing rate to the data and the old model\n  - what do we see?\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# all this code is the same as above\nhistory_pred_fr = history_model.predict(current_history)\nhistory_pred_fr = history_pred_fr / bin_size\nsmooth_history_pred_fr = history_pred_fr.dropna().smooth(.05, size_factor=20)\nplotting.current_injection_plot(current, spikes, firing_rate,\n                                               # compare against the old firing rate\n                                               smooth_history_pred_fr, smooth_predicted_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are only some small changes here. Our new model maintains the\ntwo successes of the old one: firing rate increases with injected current and shows\nthe observed periodicity. Our model has not improved the match between the firing rate\nin the first or second intervals, but it seems to do a better job of capturing the\nonset transience, especially in the third interval.\n\nWe can similarly examine our mean firing rate and the tuning curves we examined before:\n\n<div class=\"notes\">\n  - examine the predicted average firing rate and tuning curve\n  - what do we see?\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# compare observed mean firing rate with the history_model predicted one\nprint(f\"Observed mean firing rate: {np.mean(count) / bin_size} Hz\")\nprint(f\"Predicted mean firing rate (instantaneous current): {np.nanmean(predicted_fr)} Hz\")\nprint(f\"Predicted mean firing rate (current history): {np.nanmean(smooth_history_pred_fr)} Hz\")\n\ntuning_curve_history_model = nap.compute_1d_tuning_curves_continuous(smooth_history_pred_fr, current, 15)\nfig = plotting.tuning_curve_plot(tuning_curve)\nfig.axes[0].plot(tuning_curve_history_model, color=\"tomato\", label=\"glm (current history)\")\nfig.axes[0].plot(tuning_curve_model, color=\"tomato\", linestyle='--', label=\"glm (instantaneous current)\")\nfig.axes[0].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This new model is actually doing a worse job matching the mean firing rate. Looking at\nthe tuning curve, it looks like this model does predict response saturation, at about\nthe right level, and it seems to do a better job at the lower current levels , though\nits maximum firing is far too low.\n\nComparing the two models by examining their predictions is important, but you may also\nwant a number with which to evaluate and compare your models' performance. As\ndiscussed earlier, the GLM optimizes log-likelihood to find the best-fitting\nweights, and we can calculate this number using its `score` method:\n\n<div class=\"notes\">\n  - use log-likelihood to compare models\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_likelihood = model.score(predictor, count, score_type=\"log-likelihood\")\nprint(f\"log-likelihood (instantaneous current): {log_likelihood}\")\nlog_likelihood = history_model.score(current_history, count, score_type=\"log-likelihood\")\nprint(f\"log-likelihood (current history): {log_likelihood}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This log-likelihood is un-normalized and thus doesn't mean that much by\nitself, other than \"higher=better\". When comparing alternative GLMs fit on\nthe same dataset, whether that's models using different regularizers and\nsolvers or those using different predictors, comparing log-likelihoods is a\nreasonable thing to do.\n\n!!! info\n\n    Under the hood, NeMoS is minimizing the negative log-likelihood, as is\n    typical in many optimization contexts. `score` returns the real\n    log-likelihood, however, and thus higher is better.\n\nThus, we can see that, judging by the log-likelihood, the addition of the current\nhistory to the model does slightly improve it. However, notice that we increased our\nnumber of parameters tenfold, and only found a small improvement in performance.\nIncreasing the number of parameters makes you more susceptible to overfitting &mdash;\nis this tradeoff worth it? To properly answer this question, one should split the\ndataset into test and train sets, training the model on one subset of the data and\ntesting it on another to test the model's generalizability. We'll see a simple version\nof this in the next exercise, and a more streamlined version, using `scikit-learn`'s\npipelining and cross-validation machinery, will be presented in an advanced exercise.\n\n### Finishing up\n\nNote that, because the log-likelihood is un-normalized, it should not be compared\nacross datasets (because e.g., it won't account for difference in noise levels). We\nprovide the ability to compute the pseudo-$R^2$ for this purpose:\n\n<div class=\"notes\">\n  - what if you want to compare models across datasets?\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2 = model.score(predictor, count, score_type='pseudo-r2-Cohen')\nprint(f\"pseudo-r2 (instantaneous current): {r2}\")\nr2 = history_model.score(current_history, count, score_type='pseudo-r2-Cohen')\nprint(f\"pseudo-r2 (current history): {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, you might be wondering how to simulate spikes &mdash; the GLM is a LNP\nmodel, but the firing rate is just the output of *LN*, its first two steps. The firing\nrate is just the mean of a Poisson process, so we can pass it to `jax.random.poisson`:\n\n<div class=\"notes\">\n  - what about spiking?\n</div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spikes = jax.random.poisson(jax.random.PRNGKey(123), predicted_fr.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this is not actually that informative and, in general, it is\nrecommended that you focus on firing rates when interpreting your model.\n\nAlso, while\nincluding spike history is often helpful, it can sometimes make simulations unstable:\nif your GLM includes auto-regressive inputs (e.g., neurons are\nconnected to themselves or each other), simulations can sometimes can behave\npoorly because of runaway excitation [^1] [^2].\n\n[^1]: Arribas, Diego, Yuan Zhao, and Il Memming Park. \"Rescuing neural spike train\nmodels from bad MLE.\" Advances in Neural Information Processing Systems 33 (2020):\n2293-2303.\n[^2]: Hocker, David, and Memming Park. \"Multistep inference for generalized linear\nspiking models curbs runaway excitation.\" International IEEE/EMBS Conference on Neural Engineering,\nMay 2017.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Exercises {.strip-headers}\n\n<div class=\"notes\">\n  - what else can we do?\n</div>\n\nDespite the simplicity of this dataset, there is still more that we can do\nhere. The following sections provide some possible exercises to try yourself!\n\n### Other stimulation protocols\n\nWe've only fit the model to a single stimulation protocol, but our dataset\ncontains many more! How does the model perform on \"Ramp\"? On \"Noise 2\"? Based\non the example code above, write new code that fits the model on some other\nstimulation protocol and evaluate its performance. Which stimulation does it\nperform best on? Which is the worst?\n\n### Train and test sets\n\nIn this example, we've used been fitting and evaluating our model on the same\ndata set. That's generally a bad idea! Try splitting the data in to train and\ntest sets, fitting the model to one portion of the data and evaluating on\nanother portion. You could split this stimulation protocol into train and\ntest sets or use different protocols to train and test on.\n\n### Model extensions\n\nEven our extended model did not do a good job capturing the onset transience seen in\nthe data, and we could probably improve the match between the amplitudes of the\npredicted firing rate and smoothed spike train. How would we do that?\n\nWe could try adding the following inputs to the model, alone or together:\n\n- Tinkering with the current history: we tried adding the current history to the\n  model, but we only investigated one set of choices with the basis functions. What if\n  we tried changing the duration of time we considered\n  (`current_history_duration_sec`)? Different numbers of basis functions? A different\n  choice for the `Basis` object altogether? What effects would these have on our model?\n\n- Spiking history: we know neurons have a refactory period (they are unable to spike a\n  second time immediately after spiking), so maybe making the model aware of whether\n  the neuron spiked recently could help better capture the onset transience.\n\n- More complicated tuning curve: as we saw with the tuning curve plots, neither model\n  explored here quite accurately captures the relationship between the current and the\n  firing rate. Can we improve that somehow? We saw that adding the current history\n  changed this relationship, but we can also change it without including the history\n  by using a basis object in `\"eval\"` mode.\n\n## Data citation {.keep-text}\n\nThe data used in this tutorial is from the Allen Brain Map, with the\n[following\ncitation](https://knowledge.brain-map.org/data/1HEYEW7GMUKWIQW37BO/summary):\n\n**Contributors**: Agata Budzillo, Bosiljka Tasic, Brian R. Lee, Fahimeh\nBaftizadeh, Gabe Murphy, Hongkui Zeng, Jim Berg, Nathan Gouwens, Rachel\nDalley, Staci A. Sorensen, Tim Jarsky, Uygar S\u00fcmb\u00fcl Zizhen Yao\n\n**Dataset**: Allen Institute for Brain Science (2020). Allen Cell Types Database\n-- Mouse Patch-seq [dataset]. Available from\nbrain-map.org/explore/classes/multimodal-characterization.\n\n**Primary publication**: Gouwens, N.W., Sorensen, S.A., et al. (2020). Integrated\nmorphoelectric and transcriptomic classification of cortical GABAergic cells.\nCell, 183(4), 935-953.E19. https://doi.org/10.1016/j.cell.2020.09.057\n\n**Patch-seq protocol**: Lee, B. R., Budzillo, A., et al. (2021). Scaled, high\nfidelity electrophysiological, morphological, and transcriptomic cell\ncharacterization. eLife, 2021;10:e65482. https://doi.org/10.7554/eLife.65482\n\n**Mouse VISp L2/3 glutamatergic neurons**: Berg, J., Sorensen, S. A., Miller, J.,\nTing, J., et al. (2021) Human neocortical expansion involves glutamatergic\nneuron diversification. Nature, 598(7879):151-158. doi:\n10.1038/s41586-021-03813-8\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}